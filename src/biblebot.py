from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
import os
import sys
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pinecone import Pinecone, ServerlessSpec
from langchain_pinecone import PineconeVectorStore
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.callbacks import StreamingStdOutCallbackHandler
from langchain.prompts import PromptTemplate
import time
from typing import Optional, List, Dict, Any
import traceback


class BibleQAConfig:
    """Configuration class for Bible Q&A system."""
    
    def __init__(self):
        self.embedding_dimension = 768
        self.chunk_size = 1500
        self.chunk_overlap = 300
        self.batch_size = 30
        self.retriever_k = 15
        self.llm_temperature = 0.7
        self.llm_model = "gemini-2.5-flash"
        self.embedding_model = "models/embedding-001"
        self.bible_file_path = "./bible_read.txt"
        self.text_separators = ["\n\n", "\n", ". ", " ", ""]
        
    @classmethod
    def from_env(cls):
        """Create config from environment variables."""
        config = cls()
        # Override defaults with environment variables if they exist
        config.chunk_size = int(os.getenv("CHUNK_SIZE", config.chunk_size))
        config.batch_size = int(os.getenv("BATCH_SIZE", config.batch_size))
        config.retriever_k = int(os.getenv("RETRIEVER_K", config.retriever_k))
        config.bible_file_path = os.getenv("BIBLE_FILE_PATH", config.bible_file_path)
        return config


class EnvironmentValidator:
    """Validates environment variables and file dependencies."""
    
    REQUIRED_ENV_VARS = [
        "GOOGLE_API_KEY",
        "PINECONE_API_KEY", 
        "PINECONE_ENVIRONMENT",
        "PINECONE_INDEX_NAME"
    ]
    
    @classmethod
    def validate_environment(cls) -> bool:
        """Validate that all required environment variables are set."""
        missing_vars = []
        for var in cls.REQUIRED_ENV_VARS:
            if not os.getenv(var):
                missing_vars.append(var)
        
        if missing_vars:
            print("‚ùå Missing required environment variables:")
            for var in missing_vars:
                print(f"   ‚Ä¢ {var}")
            print("\nüí° Please check your .env file and ensure all variables are set.")
            return False
        
        print("‚úÖ All environment variables validated!")
        return True
    
    @classmethod
    def validate_file(cls, filepath: str) -> bool:
        """Check if the required Bible text file exists and is readable."""
        if not os.path.exists(filepath):
            print(f"‚ùå Error: {filepath} not found!")
            print("üí° Please ensure the Bible text file is in the current directory.")
            print(f"   Expected location: {os.path.abspath(filepath)}")
            return False
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                f.read(100)  # Try to read first 100 chars
            print(f"‚úÖ Bible text file found and readable: {filepath}")
            return True
        except UnicodeDecodeError:
            print(f"‚ùå File encoding error: {filepath}")
            print("üí° Please ensure the file is saved in UTF-8 encoding.")
            return False
        except Exception as e:
            print(f"‚ùå Error reading file {filepath}: {e}")
            return False


class PineconeManager:
    """Manages Pinecone vector database operations."""
    
    def __init__(self, api_key: str, index_name: str, embedding_dimension: int = 768):
        self.api_key = api_key
        self.index_name = index_name
        self.embedding_dimension = embedding_dimension
        self.pinecone_client = None
        self.index = None
    
    def initialize(self) -> bool:
        """Initialize Pinecone client and index."""
        try:
            print("üîß Initializing Pinecone...")
            self.pinecone_client = Pinecone(api_key=self.api_key)
            print("‚úÖ Pinecone client initialized!")
            
            if not self._ensure_index_exists():
                return False
                
            self.index = self.pinecone_client.Index(self.index_name)
            
            # Test the connection
            stats = self.index.describe_index_stats()
            print(f"üìä Index stats: {stats.total_vector_count} vectors")
            return True
            
        except Exception as e:
            print(f"‚ùå Pinecone initialization failed: {e}")
            print("üí° Please check your PINECONE_API_KEY and network connection.")
            return False
    
    def _ensure_index_exists(self) -> bool:
        """Ensure the Pinecone index exists, create if necessary."""
        try:
            existing_indexes = [index.name for index in self.pinecone_client.list_indexes()]
            print(f"üìã Found {len(existing_indexes)} existing indexes")
            
            if self.index_name not in existing_indexes:
                print(f"üìä Creating new Pinecone index: {self.index_name}")
                self.pinecone_client.create_index(
                    name=self.index_name,
                    dimension=self.embedding_dimension,
                    metric="cosine",
                    spec=ServerlessSpec(cloud="aws", region="us-east-1")
                )
                print("‚úÖ Index created successfully!")
            else:
                print(f"‚úÖ Using existing index: {self.index_name}")
            
            return True
            
        except Exception as e:
            if "ALREADY_EXISTS" in str(e):
                print(f"‚úÖ Index '{self.index_name}' already exists, continuing...")
                return True
            else:
                print(f"‚ùå Error creating index: {e}")
                return False
    
    def get_vector_count(self) -> int:
        """Get the current vector count in the index."""
        try:
            stats = self.index.describe_index_stats()
            return stats.total_vector_count
        except Exception as e:
            print(f"‚ùå Error getting vector count: {e}")
            return 0
    
    def get_stats(self) -> Dict[str, Any]:
        """Get detailed index statistics."""
        try:
            return self.index.describe_index_stats()
        except Exception as e:
            print(f"‚ùå Error getting stats: {e}")
            return {}


class DocumentProcessor:
    """Handles document loading, splitting, and embedding operations."""
    
    def __init__(self, config: BibleQAConfig):
        self.config = config
        self.embeddings = None
        
    def initialize_embeddings(self, google_api_key: str) -> bool:
        """Initialize Google embeddings."""
        try:
            print("üîß Initializing Google embeddings...")
            self.embeddings = GoogleGenerativeAIEmbeddings(
                model=self.config.embedding_model,
                google_api_key=google_api_key
            )
            
            # Test the embeddings
            test_result = self.embeddings.embed_query("Test embedding")
            print(f"‚úÖ Embeddings initialized! Dimension: {len(test_result)}")
            return True
            
        except Exception as e:
            print(f"‚ùå Embeddings initialization failed: {e}")
            print("üí° Please check your GOOGLE_API_KEY and internet connection.")
            return False
    
    def load_and_split_documents(self, filepath: str) -> Optional[List]:
        """Load and split documents from file."""
        try:
            print("üìñ Loading Bible text...")
            loader = TextLoader(filepath)
            documents = loader.load()
            
            if not documents:
                print("‚ùå No documents loaded!")
                return None
            
            # Get file info
            file_size = os.path.getsize(filepath)
            print(f"üìÅ File size: {file_size / 1024 / 1024:.1f} MB")
            print(f"üìÑ Loaded {len(documents)} document(s)")
            
            print("üìù Splitting text into chunks...")
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.config.chunk_size,
                chunk_overlap=self.config.chunk_overlap,
                separators=self.config.text_separators
            )
            
            texts = text_splitter.split_documents(documents)
            
            if not texts:
                print("‚ùå No text chunks created!")
                return None
            
            print(f"‚úÖ Created {len(texts)} text chunks")
            return texts
            
        except FileNotFoundError:
            print(f"‚ùå File not found: {filepath}")
            return None
        except Exception as e:
            print(f"‚ùå Error loading/splitting documents: {e}")
            return None
    
    def upload_documents_in_batches(self, texts: List, index_name: str) -> Optional[PineconeVectorStore]:
        """Upload documents to Pinecone in batches with error handling."""
        if not texts or not self.embeddings:
            print("‚ùå No text chunks or embeddings not initialized!")
            return None
        
        total_batches = (len(texts) + self.config.batch_size - 1) // self.config.batch_size
        print(f"üìä Uploading {len(texts)} documents in {total_batches} batches of {self.config.batch_size}...")
        
        vectorstore = None
        successful_batches = 0
        
        for i in range(0, len(texts), self.config.batch_size):
            batch = texts[i:i + self.config.batch_size]
            batch_num = (i // self.config.batch_size) + 1
            
            print(f"‚è≥ Processing batch {batch_num}/{total_batches} ({len(batch)} documents)...")
            
            if self._upload_batch(batch, index_name, batch_num, vectorstore):
                successful_batches += 1
                if vectorstore is None:  # First successful batch creates vectorstore
                    vectorstore = PineconeVectorStore(
                        index_name=index_name,
                        embedding=self.embeddings
                    )
            
            # Delay between batches
            if batch_num < total_batches:
                time.sleep(1)
        
        if successful_batches > 0:
            print(f"üéâ Upload completed! {successful_batches}/{total_batches} batches successful")
            return vectorstore
        else:
            print("‚ùå No batches were successfully uploaded!")
            return None
    
    def _upload_batch(self, batch: List, index_name: str, batch_num: int, 
                     existing_vectorstore: Optional[PineconeVectorStore]) -> bool:
        """Upload a single batch with retry logic."""
        max_retries = 3
        
        for retry in range(max_retries):
            try:
                if existing_vectorstore is None:
                    # First batch - create the vectorstore
                    PineconeVectorStore.from_documents(
                        batch, self.embeddings, index_name=index_name
                    )
                else:
                    # Subsequent batches - add to existing vectorstore
                    existing_vectorstore.add_documents(batch)
                
                print(f"‚úÖ Batch {batch_num} uploaded successfully!")
                return True
                
            except Exception as e:
                print(f"‚ö†Ô∏è  Batch {batch_num} failed (attempt {retry + 1}/{max_retries}): {e}")
                if retry < max_retries - 1:
                    wait_time = (retry + 1) * 2
                    print(f"   Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
                else:
                    print(f"‚ùå Batch {batch_num} failed after {max_retries} attempts")
        
        return False


class LLMManager:
    """Manages the Language Model and conversation chain."""
    
    def __init__(self, config: BibleQAConfig):
        self.config = config
        self.llm = None
        self.memory = None
        self.qa_chain = None
        
    def initialize_llm(self, google_api_key: str) -> bool:
        """Initialize Gemini LLM."""
        try:
            print("ü§ñ Initializing Gemini LLM...")
            self.llm = ChatGoogleGenerativeAI(
                google_api_key=google_api_key,
                streaming=True,
                convert_system_message_to_human=True,
                model=self.config.llm_model,
                temperature=self.config.llm_temperature,
                callbacks=[StreamingStdOutCallbackHandler()]
            )
            
            # Test the LLM
            test_response = self.llm.invoke("Say 'Hello' in Chinese")
            print(f"‚úÖ LLM initialized! Test response: {test_response.content[:50]}...")
            return True
            
        except Exception as e:
            print(f"‚ùå LLM initialization failed: {e}")
            print("üí° Please check your GOOGLE_API_KEY and model availability.")
            return False
    
    def initialize_memory(self) -> bool:
        """Initialize conversation memory."""
        try:
            self.memory = ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
                output_key="answer"
            )
            print("‚úÖ Memory initialized")
            return True
        except Exception as e:
            print(f"‚ùå Error initializing memory: {e}")
            return False
    
    def create_qa_chain(self, retriever) -> bool:
        """Create the conversational retrieval chain."""
        try:
            custom_prompt = self._create_pastoral_prompt()
            
            self.qa_chain = ConversationalRetrievalChain.from_llm(
                llm=self.llm,
                retriever=retriever,
                memory=self.memory,
                verbose=True,
                return_source_documents=True,
                combine_docs_chain_kwargs={"prompt": custom_prompt},
            )
            print("‚úÖ Q&A chain created successfully")
            return True
            
        except Exception as e:
            print(f"‚ùå Error creating Q&A chain: {e}")
            return False
    
    def _create_pastoral_prompt(self) -> PromptTemplate:
        """Create the pastoral guidance prompt template."""
        return PromptTemplate(
            input_variables=["context", "chat_history", "question"],
            template="""‰Ω†ÊòØ‰∏Ä‰ΩçÂçöÂ≠¶„ÄÅÂπΩÈªò„ÄÅ‰∏îÂØåÊúâÂêåÊÉÖÂøÉÁöÑÂú£ÁªèÂ≠¶ËÄÖÂíåÂ±ûÁÅµÂØºÂ∏àÔºåÂêåÊó∂‰πüÊòØ‰∏Ä‰ΩçÁªèÈ™å‰∏∞ÂØåÁöÑÁâßËÄÖ„ÄÇ‰Ω†ÁöÑËßíËâ≤ÊòØÊèê‰æõÂÖ®Èù¢„ÄÅÊ∑±ÊÄùÁÜüËôë‰∏îÂÖ∑ÊúâÂ±ûÁÅµÂêØÂèëÁöÑÂú£ÁªèÂíåÂü∫Áù£Êïô‰ø°‰ª∞Á≠îÊ°àÔºåÂπ∂‰ª•ÁâßËÄÖÁöÑÂøÉËÇ†Êù•ÂÖ≥ÊÄÄÂíåÊåáÂØºËØ¢ÈóÆËÄÖ„ÄÇ

‰Ωú‰∏∫‰∏Ä‰ΩçÂ•ΩÁâßËÄÖÔºåËØ∑ÈÅµÂæ™‰ª•‰∏ãÂéüÂàôÔºö

ÁâßËÄÖÂìÅÊ†º‰∏éÁ≠ñÁï•Ôºö
‚Ä¢ ‰ª•Âü∫Áù£ÁöÑÁà±ÂøÉÂíåËÄêÂøÉÂõûÂ∫îÊØè‰∏Ä‰∏™ÈóÆÈ¢ò
‚Ä¢ ÂÄæÂê¨Âπ∂ÁêÜËß£ËØ¢ÈóÆËÄÖÂÜÖÂøÉÁöÑÈúÄË¶ÅÂíåÊå£Êâé
‚Ä¢ Êèê‰æõÊó¢ÊúâÁúüÁêÜÂèàÊúâÊÅ©ÂÖ∏ÁöÑÂπ≥Ë°°ÂõûÁ≠î
‚Ä¢ Áî®Ê∏©ÊüîËÄåÂùöÂÆöÁöÑËØ≠Ê∞î‰º†ËææÁ•ûÁöÑËØùËØ≠
‚Ä¢ ÈÅøÂÖçËÆ∫Êñ≠ÔºåËÄåÊòØ‰ª•ÊÖàÁà±ÂºïÂØº‰∫∫ÂõûÂà∞Á•ûÈù¢Ââç
‚Ä¢ ‰∏∫ËØ¢ÈóÆËÄÖÁöÑÂ±ûÁÅµÊàêÈïøÂíåÂÆûÈôÖÈúÄË¶ÅÁ•∑ÂëäËÄÉËôë
‚Ä¢ Êèê‰æõÊó¢ÊúâÊ∑±Â∫¶ÂèàÂÆπÊòìÁêÜËß£ÁöÑËß£Èáä

ËØ∑‰ΩøÁî®‰ª•‰∏ãÂú£ÁªèÁªèÊñáÂíåËÉåÊôØÊù•ÂõûÁ≠îÈóÆÈ¢òÔºå‰∏î‰∏çË¶ÅË∂ÖÂá∫‰ª•‰∏ãÁªèÊñáÂíåËÉåÊôØÊâÄÊèê‰æõÁöÑËåÉÂõ¥ÂõûÁ≠îÔºåÂ¶ÇÊûú‰∏çÁü•ÈÅìÔºåÂ∞±ÂõûÁ≠î‰∏çÁü•ÈÅì„ÄÇËØ∑Êèê‰æõËØ¶ÁªÜ‰∏îÁªìÊûÑÊ∏ÖÊô∞ÁöÑÂõûÁ≠îÔºåÂåÖÊã¨Ôºö

1. ÂØπÈóÆÈ¢òÁöÑÁõ¥Êé•ÂõûÁ≠îÔºà‰ª•ÁâßËÄÖÁöÑÂÖ≥ÊÄÄÂºÄÂßãÔºâ
2. Áõ∏ÂÖ≥ÁöÑÂú£ÁªèÁªèÊñáÂíåÂºïÁî®ÔºàËß£ÈáäÂÖ∂‰∏≠ÁöÑÂ±ûÁÅµÂê´‰πâÔºâ
3. ÂéÜÂè≤ÂíåÊñáÂåñËÉåÊôØÔºàÂ¶ÇÈÄÇÁî®Ôºâ
4. ÂÆûÈôÖÂ∫îÁî®ÊàñÂ±ûÁÅµËßÅËß£ÔºàÈíàÂØπÁé∞‰ª£‰ø°ÂæíÁöÑÁîüÊ¥ªÔºâ
5. ‰∏éÁõ∏ÂÖ≥Âú£ÁªèÊ¶ÇÂøµÁöÑ‰∫§ÂèâÂºïÁî®
6. ÁâßËÄÖÁöÑÈºìÂä±ÂíåÁ•∑ÂëäÊñπÂêëÔºàÂ¶ÇÈÄÇÁî®Ôºâ

Âú£ÁªèËÉåÊôØÔºö
{context}

‰πãÂâçÁöÑÂØπËØùÔºö
{chat_history}

ÈóÆÈ¢òÔºö{question}

‰Ωú‰∏∫‰∏Ä‰ΩçÂ±ûÁÅµÂØºÂ∏àÔºåËØ∑Êèê‰æõ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÁ≠îÊ°àÔºåÂØπ‰∫éÈÇ£‰∫õÂØªÊ±ÇÊõ¥Â•ΩÁêÜËß£Á•ûËØùËØ≠ÁöÑ‰∫∫ÊúâÂ∏ÆÂä©„ÄÇËØ∑ÂåÖÂê´ÂÖ∑‰ΩìÁöÑÁªèÊñáÂºïÁî®ÔºåÂπ∂‰ª•Ê∏ÖÊô∞ÊòìÊáÇÁöÑÊñπÂºèËß£ÈáäÂê´‰πâ„ÄÇÂú®ÂõûÁ≠î‰∏≠‰ΩìÁé∞ÁâßËÄÖÁöÑÂÖ≥ÊÄÄ„ÄÅÊô∫ÊÖßÂíåÂ±ûÁÅµÁöÑÊ∑±Â∫¶„ÄÇËØ∑Áî®‰∏≠ÊñáÂõûÁ≠î„ÄÇ

Â¶ÇÊûúÈÅáÂà∞‰∏éÂú£Áªè‰∏çÁõ∏ÂÖ≥ÁöÑÈóÆÈ¢òÔºåËØ∑‰ª•ÁâßËÄÖÁöÑÂøÉËÇ†Ê∏©ÊüîÂú∞Â∞ÜËØùÈ¢òÂºïÂØºÂõûÂà∞Âú£ÁªèÁõ∏ÂÖ≥ÁöÑÈóÆÈ¢òÔºåÂêåÊó∂ÂÖ≥ÂøÉËØ¢ÈóÆËÄÖÁöÑÂ±ûÁÅµÈúÄË¶Å„ÄÇÈÅøÂÖç‰ΩøÁî®‰∏™‰∫∫ËßÇÁÇπÊàñÊú™ÁªèËØÅÂÆûÁöÑËß£ÈáäÔºå‰ΩÜË¶Å‰ª•ÁâßËÄÖÁöÑÁªèÈ™åÂíåÊô∫ÊÖßÊù•Â∫îÁî®Âú£ÁªèÁúüÁêÜ„ÄÇ

ËÆ∞‰ΩèÔºå‰Ω†‰∏ç‰ªÖÊòØÂú®‰º†ÊéàÁü•ËØÜÔºåÊõ¥ÊòØÂú®ÁâßÂÖªÁÅµÈ≠ÇÔºåÂ∏ÆÂä©‰∫∫‰ª¨Âú®Âü∫Áù£ÈáåÊàêÈïø„ÄÇ

ÂõûÂ§çÔºö"""
        )
    
    def process_question(self, question: str) -> Optional[Dict[str, Any]]:
        """Process a question through the QA chain."""
        if not self.qa_chain:
            print("‚ùå QA chain not initialized!")
            return None
            
        try:
            result = self.qa_chain({"question": question})
            return result
        except Exception as e:
            print(f"‚ùå Error processing question: {e}")
            return None
    
    def clear_memory(self) -> bool:
        """Clear conversation memory."""
        try:
            if self.memory:
                self.memory.clear()
                return True
            return False
        except Exception as e:
            print(f"‚ùå Error clearing memory: {e}")
            return False


class BibleQASystem:
    """Main Bible Q&A system orchestrating all components."""
    
    def __init__(self):
        self.config = BibleQAConfig.from_env()
        self.pinecone_manager = None
        self.document_processor = None
        self.llm_manager = None
        self.vectorstore = None
        self.retriever = None
        
    def initialize(self) -> bool:
        """Initialize all system components."""
        load_dotenv()
        print("üöÄ Starting Bible Q&A System...")
        print("=" * 50)
        
        # Validate environment
        if not EnvironmentValidator.validate_environment():
            return False
        
        if not EnvironmentValidator.validate_file(self.config.bible_file_path):
            return False
        
        # Initialize components
        if not self._initialize_pinecone():
            return False
            
        if not self._initialize_document_processor():
            return False
            
        if not self._setup_vectorstore():
            return False
            
        if not self._initialize_llm():
            return False
            
        print("\nüéâ Bible Q&A System initialized successfully!")
        return True
    
    def _initialize_pinecone(self) -> bool:
        """Initialize Pinecone manager."""
        self.pinecone_manager = PineconeManager(
            api_key=os.getenv("PINECONE_API_KEY"),
            index_name=os.getenv("PINECONE_INDEX_NAME"),
            embedding_dimension=self.config.embedding_dimension
        )
        return self.pinecone_manager.initialize()
    
    def _initialize_document_processor(self) -> bool:
        """Initialize document processor with embeddings."""
        self.document_processor = DocumentProcessor(self.config)
        return self.document_processor.initialize_embeddings(os.getenv("GOOGLE_API_KEY"))
    
    def _setup_vectorstore(self) -> bool:
        """Setup vectorstore with documents."""
        vector_count = self.pinecone_manager.get_vector_count()
        
        if vector_count == 0:
            print("üìä No existing vectors found. Loading and uploading documents...")
            texts = self.document_processor.load_and_split_documents(self.config.bible_file_path)
            if not texts:
                return False
                
            self.vectorstore = self.document_processor.upload_documents_in_batches(
                texts, self.pinecone_manager.index_name
            )
            if not self.vectorstore:
                return False
        else:
            print(f"‚úÖ Found {vector_count} existing vectors in Pinecone")
            self.vectorstore = PineconeVectorStore(
                index_name=self.pinecone_manager.index_name,
                embedding=self.document_processor.embeddings
            )
        
        # Create retriever
        try:
            self.retriever = self.vectorstore.as_retriever(
                search_kwargs={"k": self.config.retriever_k},
                search_type="similarity"
            )
            print("‚úÖ Retriever created successfully")
            return True
        except Exception as e:
            print(f"‚ùå Error creating retriever: {e}")
            return False
    
    def _initialize_llm(self) -> bool:
        """Initialize LLM manager and create QA chain."""
        self.llm_manager = LLMManager(self.config)
        
        if not self.llm_manager.initialize_llm(os.getenv("GOOGLE_API_KEY")):
            return False
            
        if not self.llm_manager.initialize_memory():
            return False
            
        return self.llm_manager.create_qa_chain(self.retriever)
    
    def run_interactive_session(self):
        """Run the interactive Q&A session."""
        print("\n" + "="*60)
        print("üîç Bible Q&A System Ready!")
        print("Ask questions about the Bible. Commands:")
        print("  'exit' - Quit the program")
        print("  'clear' - Clear conversation memory")
        print("  'stats' - Show Pinecone index statistics")
        print("  'help' - Show help message")
        print("="*60)
        
        while True:
            try:
                query = input("\nüìñ Your question: ").strip()
                
                if not self._handle_command(query):
                    continue
                    
                if not query:
                    continue
                
                self._process_user_question(query)
                
            except KeyboardInterrupt:
                print("\nüëã May God bless your continued study of His Word!")
                break
            except EOFError:
                print("\nüëã Goodbye!")
                break
            except Exception as e:
                print(f"‚ùå Unexpected error: {e}")
                if os.getenv("DEBUG", "").lower() == "true":
                    traceback.print_exc()
                continue
    
    def _handle_command(self, query: str) -> bool:
        """Handle special commands. Returns False if command was processed."""
        if query.lower() == "exit":
            print("üëã God bless your continued study of His Word!")
            sys.exit(0)
            
        elif query.lower() == "clear":
            if self.llm_manager.clear_memory():
                print("üßπ Memory cleared!")
            else:
                print("‚ùå Error clearing memory")
            return False
            
        elif query.lower() == "stats":
            stats = self.pinecone_manager.get_stats()
            if stats:
                print(f"üìä Index stats: {stats.total_vector_count} vectors, {stats.dimension} dimensions")
            else:
                print("‚ùå Error getting stats")
            return False
            
        elif query.lower() == "help":
            self._show_help()
            return False
            
        return True
    
    def _show_help(self):
        """Show help message."""
        print("\nüí° Available commands:")
        print("  'exit' - Quit the program")
        print("  'clear' - Clear conversation memory")
        print("  'stats' - Show Pinecone index statistics")
        print("  'help' - Show this help message")
        print("\nüìñ Example questions:")
        print("  ‚Ä¢ ‰ªÄ‰πàÊòØÁà±Ôºü")
        print("  ‚Ä¢ Âú£ÁªèÂ¶Ç‰ΩïÊïôÂØºÊàë‰ª¨Á•∑ÂëäÔºü")
        print("  ‚Ä¢ ËÄ∂Á®£ÁöÑÊØîÂñªÊúâ‰ªÄ‰πàÊÑè‰πâÔºü")
        print("  ‚Ä¢ Â¶Ç‰ΩïÁêÜËß£ÊïëÊÅ©Ôºü")
    
    def _process_user_question(self, query: str):
        """Process a user question and display results."""
        print("üîç Searching Scripture and preparing answer...")
        
        result = self.llm_manager.process_question(query)
        if not result:
            print("‚ùå No answer received from the system")
            return
        
        if "answer" in result and result["answer"]:
            print("\n" + "="*60)
            print("üìñ Biblical Answer:")
            print("="*60)
            print(result["answer"])
        else:
            print("‚ùå Empty response from AI")
            return
        
        self._display_source_documents(result.get("source_documents", []))
    
    def _display_source_documents(self, sources: List):
        """Display source documents with proper formatting."""
        if not sources:
            print("\n‚ö†Ô∏è  No source documents were retrieved for this question.")
            return
        
        print(f"\nüìö Biblical References ({len(sources)} documents):")
        print("=" * 60)
        
        for i, doc in enumerate(sources, 1):
            try:
                print(f"\nüìñ Source {i}:")
                print("-" * 30)
                content = doc.page_content.strip()
                print(content)
                
                # Show metadata if available
                if hasattr(doc, 'metadata') and doc.metadata:
                    print(f"üìã Metadata: {doc.metadata}")
                    
            except Exception as e:
                print(f"‚ùå Error displaying source {i}: {e}")
        
        print("=" * 60)


def main():
    """Main entry point."""
    try:
        bible_qa = BibleQASystem()
        
        if bible_qa.initialize():
            bible_qa.run_interactive_session()
            return 0
        else:
            print("‚ùå Failed to initialize Bible Q&A system")
            return 1
            
    except Exception as e:
        print(f"‚ùå Critical error: {e}")
        if os.getenv("DEBUG", "").lower() == "true":
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except Exception as e:
        print(f"‚ùå Fatal error: {e}")
        sys.exit(1)