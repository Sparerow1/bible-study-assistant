from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.callbacks import StreamingStdOutCallbackHandler
from langchain.memory import ConversationBufferMemory
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from config.config_setup_class import BibleQAConfig
from typing import Optional, Dict, Any

class LLMManager:
    """Manages the Language Model and conversation chain."""
    
    def __init__(self, config: BibleQAConfig):
        self.config = config
        self.llm = None
        self.memory = None
        self.qa_chain = None
        
    def initialize_llm(self, google_api_key: str) -> bool:
        """Initialize Gemini LLM."""
        try:
            print("ü§ñ Initializing Gemini LLM...")
            self.llm = ChatGoogleGenerativeAI(
                google_api_key=google_api_key,
                streaming=True,
                convert_system_message_to_human=True,
                model=self.config.llm_model,
                temperature=self.config.llm_temperature,
                callbacks=[StreamingStdOutCallbackHandler()]
            )
            
            # Test the LLM
            test_response = self.llm.invoke("Say 'Hello' in Chinese")
            print(f"‚úÖ LLM initialized! Test response: {test_response.content[:50]}...")
            return True
            
        except Exception as e:
            print(f"‚ùå LLM initialization failed: {e}")
            print("üí° Please check your GOOGLE_API_KEY and model availability.")
            return False
    
    def initialize_memory(self) -> bool:
        """Initialize conversation memory."""
        try:
            self.memory = ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
                output_key="answer"
            )
            print("‚úÖ Memory initialized")
            return True
        except Exception as e:
            print(f"‚ùå Error initializing memory: {e}")
            return False
    
    def create_qa_chain(self, retriever) -> bool:
        """Create the conversational retrieval chain."""
        try:
            custom_prompt = self._create_pastoral_prompt()
            
            self.qa_chain = ConversationalRetrievalChain.from_llm(
                llm=self.llm,
                retriever=retriever,
                memory=self.memory,
                verbose=True,
                return_source_documents=True,
                combine_docs_chain_kwargs={"prompt": custom_prompt},
            )
            print("‚úÖ Q&A chain created successfully")
            return True
            
        except Exception as e:
            print(f"‚ùå Error creating Q&A chain: {e}")
            return False
    
    def _create_pastoral_prompt(self) -> PromptTemplate:
        """Create the pastoral guidance prompt template."""
        return PromptTemplate(
            input_variables=["context", "chat_history", "question"],
            template="""‰Ω†ÊòØ‰∏Ä‰ΩçÂçöÂ≠¶„ÄÅÂπΩÈªò„ÄÅ‰∏îÂØåÊúâÂêåÊÉÖÂøÉÁöÑÂú£ÁªèÂ≠¶ËÄÖÂíåÂ±ûÁÅµÂØºÂ∏àÔºåÂêåÊó∂‰πüÊòØ‰∏Ä‰ΩçÁªèÈ™å‰∏∞ÂØåÁöÑÁâßËÄÖ„ÄÇ‰Ω†ÁöÑËßíËâ≤ÊòØÊèê‰æõÂÖ®Èù¢„ÄÅÊ∑±ÊÄùÁÜüËôë‰∏îÂÖ∑ÊúâÂ±ûÁÅµÂêØÂèëÁöÑÂú£ÁªèÂíåÂü∫Áù£Êïô‰ø°‰ª∞Á≠îÊ°àÔºåÂπ∂‰ª•ÁâßËÄÖÁöÑÂøÉËÇ†Êù•ÂÖ≥ÊÄÄÂíåÊåáÂØºËØ¢ÈóÆËÄÖ„ÄÇ

‰Ωú‰∏∫‰∏Ä‰ΩçÂ•ΩÁâßËÄÖÔºåËØ∑ÈÅµÂæ™‰ª•‰∏ãÂéüÂàôÔºö

ÁâßËÄÖÂìÅÊ†º‰∏éÁ≠ñÁï•Ôºö
‚Ä¢ ‰ª•Âü∫Áù£ÁöÑÁà±ÂøÉÂíåËÄêÂøÉÂõûÂ∫îÊØè‰∏Ä‰∏™ÈóÆÈ¢ò
‚Ä¢ ÂÄæÂê¨Âπ∂ÁêÜËß£ËØ¢ÈóÆËÄÖÂÜÖÂøÉÁöÑÈúÄË¶ÅÂíåÊå£Êâé
‚Ä¢ Êèê‰æõÊó¢ÊúâÁúüÁêÜÂèàÊúâÊÅ©ÂÖ∏ÁöÑÂπ≥Ë°°ÂõûÁ≠î
‚Ä¢ Áî®Ê∏©ÊüîËÄåÂùöÂÆöÁöÑËØ≠Ê∞î‰º†ËææÁ•ûÁöÑËØùËØ≠
‚Ä¢ ÈÅøÂÖçËÆ∫Êñ≠ÔºåËÄåÊòØ‰ª•ÊÖàÁà±ÂºïÂØº‰∫∫ÂõûÂà∞Á•ûÈù¢Ââç
‚Ä¢ ‰∏∫ËØ¢ÈóÆËÄÖÁöÑÂ±ûÁÅµÊàêÈïøÂíåÂÆûÈôÖÈúÄË¶ÅÁ•∑ÂëäËÄÉËôë
‚Ä¢ Êèê‰æõÊó¢ÊúâÊ∑±Â∫¶ÂèàÂÆπÊòìÁêÜËß£ÁöÑËß£Èáä

ËØ∑‰ΩøÁî®‰ª•‰∏ãÂú£ÁªèÁªèÊñáÂíåËÉåÊôØÊù•ÂõûÁ≠îÈóÆÈ¢òÔºå‰∏î‰∏çË¶ÅË∂ÖÂá∫‰ª•‰∏ãÁªèÊñáÂíåËÉåÊôØÊâÄÊèê‰æõÁöÑËåÉÂõ¥ÂõûÁ≠îÔºåÂ¶ÇÊûú‰∏çÁü•ÈÅìÔºåÂ∞±ÂõûÁ≠î‰∏çÁü•ÈÅì„ÄÇËØ∑Êèê‰æõËØ¶ÁªÜ‰∏îÁªìÊûÑÊ∏ÖÊô∞ÁöÑÂõûÁ≠îÔºåÂåÖÊã¨Ôºö

1. ÂØπÈóÆÈ¢òÁöÑÁõ¥Êé•ÂõûÁ≠îÔºà‰ª•ÁâßËÄÖÁöÑÂÖ≥ÊÄÄÂºÄÂßãÔºâ
2. Áõ∏ÂÖ≥ÁöÑÂú£ÁªèÁªèÊñáÂíåÂºïÁî®ÔºàËß£ÈáäÂÖ∂‰∏≠ÁöÑÂ±ûÁÅµÂê´‰πâÔºâ
3. ÂéÜÂè≤ÂíåÊñáÂåñËÉåÊôØÔºàÂ¶ÇÈÄÇÁî®Ôºâ
4. ÂÆûÈôÖÂ∫îÁî®ÊàñÂ±ûÁÅµËßÅËß£ÔºàÈíàÂØπÁé∞‰ª£‰ø°ÂæíÁöÑÁîüÊ¥ªÔºâ
5. ‰∏éÁõ∏ÂÖ≥Âú£ÁªèÊ¶ÇÂøµÁöÑ‰∫§ÂèâÂºïÁî®
6. ÁâßËÄÖÁöÑÈºìÂä±ÂíåÁ•∑ÂëäÊñπÂêëÔºàÂ¶ÇÈÄÇÁî®Ôºâ

Âú£ÁªèËÉåÊôØÔºö
{context}

‰πãÂâçÁöÑÂØπËØùÔºö
{chat_history}

ÈóÆÈ¢òÔºö{question}

‰Ωú‰∏∫‰∏Ä‰ΩçÂ±ûÁÅµÂØºÂ∏àÔºåËØ∑Êèê‰æõ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÁ≠îÊ°àÔºåÂØπ‰∫éÈÇ£‰∫õÂØªÊ±ÇÊõ¥Â•ΩÁêÜËß£Á•ûËØùËØ≠ÁöÑ‰∫∫ÊúâÂ∏ÆÂä©„ÄÇËØ∑ÂåÖÂê´ÂÖ∑‰ΩìÁöÑÁªèÊñáÂºïÁî®ÔºåÂπ∂‰ª•Ê∏ÖÊô∞ÊòìÊáÇÁöÑÊñπÂºèËß£ÈáäÂê´‰πâ„ÄÇÂú®ÂõûÁ≠î‰∏≠‰ΩìÁé∞ÁâßËÄÖÁöÑÂÖ≥ÊÄÄ„ÄÅÊô∫ÊÖßÂíåÂ±ûÁÅµÁöÑÊ∑±Â∫¶„ÄÇËØ∑Áî®‰∏≠ÊñáÂõûÁ≠î„ÄÇ

Â¶ÇÊûúÈÅáÂà∞‰∏éÂú£Áªè‰∏çÁõ∏ÂÖ≥ÁöÑÈóÆÈ¢òÔºåËØ∑‰ª•ÁâßËÄÖÁöÑÂøÉËÇ†Ê∏©ÊüîÂú∞Â∞ÜËØùÈ¢òÂºïÂØºÂõûÂà∞Âú£ÁªèÁõ∏ÂÖ≥ÁöÑÈóÆÈ¢òÔºåÂêåÊó∂ÂÖ≥ÂøÉËØ¢ÈóÆËÄÖÁöÑÂ±ûÁÅµÈúÄË¶Å„ÄÇÈÅøÂÖç‰ΩøÁî®‰∏™‰∫∫ËßÇÁÇπÊàñÊú™ÁªèËØÅÂÆûÁöÑËß£ÈáäÔºå‰ΩÜË¶Å‰ª•ÁâßËÄÖÁöÑÁªèÈ™åÂíåÊô∫ÊÖßÊù•Â∫îÁî®Âú£ÁªèÁúüÁêÜ„ÄÇ

ËÆ∞‰ΩèÔºå‰Ω†‰∏ç‰ªÖÊòØÂú®‰º†ÊéàÁü•ËØÜÔºåÊõ¥ÊòØÂú®ÁâßÂÖªÁÅµÈ≠ÇÔºåÂ∏ÆÂä©‰∫∫‰ª¨Âú®Âü∫Áù£ÈáåÊàêÈïø„ÄÇ

ÂõûÂ§çÔºö"""
        )
    
    def process_question(self, question: str) -> Optional[Dict[str, Any]]:
        """Process a question through the QA chain."""
        if not self.qa_chain:
            print("‚ùå QA chain not initialized!")
            return None
            
        try:
            result = self.qa_chain({"question": question})
            return result
        except Exception as e:
            print(f"‚ùå Error processing question: {e}")
            return None
    
    def clear_memory(self) -> bool:
        """Clear conversation memory."""
        try:
            if self.memory:
                self.memory.clear()
                return True
            return False
        except Exception as e:
            print(f"‚ùå Error clearing memory: {e}")
            return False