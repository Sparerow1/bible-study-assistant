from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
import os
import sys
from typing import Optional, List, Dict, Any
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pinecone import Pinecone, ServerlessSpec
from langchain_pinecone import PineconeVectorStore
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.callbacks import StreamingStdOutCallbackHandler
from langchain.prompts import PromptTemplate
import time
import traceback


class BibleBotError(Exception):
    """Custom exception for Bible Bot errors."""
    pass


class EnvironmentValidator:
    """Validates environment variables and setup."""
    
    REQUIRED_VARS = [
        "GOOGLE_API_KEY",
        "PINECONE_API_KEY", 
        "PINECONE_ENVIRONMENT",
        "PINECONE_INDEX_NAME"
    ]
    
    @classmethod
    def validate(cls) -> bool:
        """Validate that all required environment variables are set."""
        missing_vars = [var for var in cls.REQUIRED_VARS if not os.getenv(var)]
        
        if missing_vars:
            print("‚ùå Missing required environment variables:")
            for var in missing_vars:
                print(f"   ‚Ä¢ {var}")
            print("\nüí° Please check your .env file and ensure all variables are set.")
            return False
        
        print("‚úÖ All environment variables validated!")
        return True


class DocumentManager:
    """Handles document loading, processing, and validation."""
    
    def __init__(self, file_path: str = "./bible_read.txt"):
        self.file_path = file_path
        self.documents = None
        self.text_chunks = None
    
    def validate_file(self) -> bool:
        """Check if the Bible text file exists and is readable."""
        if not os.path.exists(self.file_path):
            print(f"‚ùå Error: {self.file_path} not found!")
            print("üí° Please ensure the Bible text file is in the current directory.")
            print(f"   Expected location: {os.path.abspath(self.file_path)}")
            return False
        
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                f.read(100)
            print(f"‚úÖ Bible text file found and readable: {self.file_path}")
            return True
        except UnicodeDecodeError:
            print(f"‚ùå File encoding error: {self.file_path}")
            print("üí° Please ensure the file is saved in UTF-8 encoding.")
            return False
        except Exception as e:
            print(f"‚ùå Error reading file {self.file_path}: {e}")
            return False
    
    def load_documents(self) -> bool:
        """Load documents from the file."""
        try:
            print("üìñ Loading Bible text...")
            loader = TextLoader(self.file_path)
            self.documents = loader.load()
            
            if not self.documents:
                raise BibleBotError("No documents loaded!")
            
            file_size = os.path.getsize(self.file_path)
            print(f"üìÅ File size: {file_size / 1024 / 1024:.1f} MB")
            print(f"üìÑ Loaded {len(self.documents)} document(s)")
            return True
            
        except Exception as e:
            print(f"‚ùå Error loading documents: {e}")
            return False
    
    def split_documents(self, chunk_size: int = 1500, chunk_overlap: int = 300) -> bool:
        """Split documents into chunks."""
        if not self.documents:
            print("‚ùå No documents to split! Load documents first.")
            return False
        
        try:
            print("üìù Splitting text into chunks...")
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separators=["\n\n", "\n", ". ", " ", ""]
            )
            
            self.text_chunks = text_splitter.split_documents(self.documents)
            
            if not self.text_chunks:
                raise BibleBotError("No text chunks created!")
            
            print(f"‚úÖ Created {len(self.text_chunks)} text chunks")
            return True
            
        except Exception as e:
            print(f"‚ùå Error splitting documents: {e}")
            return False


class PineconeManager:
    """Manages Pinecone vector database operations."""
    
    def __init__(self, api_key: str, index_name: str, dimension: int = 768):
        self.api_key = api_key
        self.index_name = index_name
        self.dimension = dimension
        self.pinecone_client = None
        self.index = None
        self.vectorstore = None
    
    def initialize(self) -> bool:
        """Initialize Pinecone client and index."""
        try:
            print("üîß Initializing Pinecone...")
            self.pinecone_client = Pinecone(api_key=self.api_key)
            print("‚úÖ Pinecone client initialized!")
            
            return self._setup_index()
            
        except Exception as e:
            print(f"‚ùå Pinecone initialization failed: {e}")
            return False
    
    def _setup_index(self) -> bool:
        """Setup or connect to Pinecone index."""
        try:
            existing_indexes = [index.name for index in self.pinecone_client.list_indexes()]
            print(f"üìã Found {len(existing_indexes)} existing indexes")
            
            if self.index_name not in existing_indexes:
                print(f"üìä Creating new Pinecone index: {self.index_name}")
                self.pinecone_client.create_index(
                    name=self.index_name,
                    dimension=self.dimension,
                    metric="cosine",
                    spec=ServerlessSpec(cloud="aws", region="us-east-1")
                )
                print("‚úÖ Index created successfully!")
            else:
                print(f"‚úÖ Using existing index: {self.index_name}")
            
            self.index = self.pinecone_client.Index(self.index_name)
            stats = self.index.describe_index_stats()
            print(f"üìä Index stats: {stats.total_vector_count} vectors")
            return True
            
        except Exception as e:
            if "ALREADY_EXISTS" in str(e):
                print(f"‚úÖ Index '{self.index_name}' already exists, continuing...")
                self.index = self.pinecone_client.Index(self.index_name)
                return True
            else:
                print(f"‚ùå Error setting up index: {e}")
                return False
    
    def get_vector_count(self) -> int:
        """Get current vector count in the index."""
        if not self.index:
            return 0
        try:
            stats = self.index.describe_index_stats()
            return stats.total_vector_count
        except Exception as e:
            print(f"‚ùå Error getting vector count: {e}")
            return 0
    
    def upload_documents_in_batches(self, texts: List, embeddings, batch_size: int = 30) -> bool:
        """Upload documents to Pinecone in smaller batches to avoid size limits."""
        total_batches = (len(texts) + batch_size - 1) // batch_size
        print(f"üìä Uploading {len(texts)} documents in {total_batches} batches of {batch_size}...")
        
        vectorstore = None
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            
            print(f"‚è≥ Processing batch {batch_num}/{total_batches} ({len(batch)} documents)...")
            
            try:
                if vectorstore is None:
                    # First batch - create the vectorstore
                    vectorstore = PineconeVectorStore.from_documents(
                        batch,
                        embeddings,
                        index_name=self.index_name,
                    )
                else:
                    # Subsequent batches - add to existing vectorstore
                    vectorstore.add_documents(batch)
                
                print(f"‚úÖ Batch {batch_num} uploaded successfully!")
                
                # Small delay to avoid rate limiting
                if batch_num < total_batches:
                    time.sleep(1)
                    
            except Exception as e:
                print(f"‚ùå Error uploading batch {batch_num}: {e}")
                return None
        
        print("üéâ All documents uploaded successfully!")
        return vectorstore

def main():   
    load_dotenv()  # Load environment variables from .env file
    
    # Load environment variables and set up Pinecone
    pinecone = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))

    embedding_dimension = 768 
    index_name = os.getenv("PINECONE_INDEX_NAME", "biblebot-index")
    
    # Check if index exists, and create if not
    existing_indexes = [index.name for index in pinecone.list_indexes()]
    
    if index_name not in existing_indexes:
        print(f"üìä Creating new Pinecone index: {index_name}")
        try:
            pinecone.create_index(
                name=index_name,
                dimension=embedding_dimension,
                metric="cosine",
                spec=ServerlessSpec(cloud="aws", region="us-east-1")
            )
            print("‚úÖ Index created successfully!")
        except Exception as e:
            if "ALREADY_EXISTS" in str(e):
                print(f"‚úÖ Index '{index_name}' already exists, continuing...")
            else:
                print(f"‚ùå Error creating index: {e}")
                return
    else:
        print(f"‚úÖ Using existing index: {index_name}")
    
    # Connect to the Pinecone index
    index = pinecone.Index(index_name)

    # Check if bible_read.txt exists
    if not os.path.exists("./bible_read.txt"):
        print("‚ùå Error: bible_read.txt not found!")
        print("Please make sure the Bible text file is in the current directory.")
        return

    # 1. Document Loading and Processing
    print("üìñ Loading Bible text...")
    loader = TextLoader("./bible_read.txt")
    documents = loader.load()
    
    # Get file size for reference
    file_size = os.path.getsize("./bible_read.txt")
    print(f"üìÅ File size: {file_size / 1024 / 1024:.1f} MB")

    print(f"üìù Splitting text into chunks...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,  # Smaller chunks to reduce batch size
        chunk_overlap=300,
        separators=["\n\n", "\n", ". ", " ", ""] 
    )
    texts = text_splitter.split_documents(documents)
    print(f"‚úÖ Created {len(texts)} text chunks")

    # Initialize embeddings
    print("üîß Initializing Google embeddings...")
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=os.getenv("GOOGLE_API_KEY")
    )
    print("‚úÖ Embeddings initialized!")

    # 2. Check if vectors already exist in Pinecone
    index_stats = index.describe_index_stats()
    vector_count = index_stats.total_vector_count
    
    if vector_count == 0:
        # Upload documents in batches
        vectorstore = upload_documents_in_batches(texts, embeddings, index_name, batch_size=30)
        if vectorstore is None:
            print("‚ùå Failed to upload documents!")
            return
    else:
        print(f"‚úÖ Found {vector_count} existing vectors in Pinecone")
        # Create vectorstore connection to existing data
        vectorstore = PineconeVectorStore(
            index_name=index_name, 
            embedding=embeddings
        )

    # Create a retriever from the Pinecone index
    retriever = vectorstore.as_retriever(search_kwargs={"k": 15},
                                         search_type="similarity",)

    # 3. Conversational Retrieval Chain Implementation
    print("ü§ñ Initializing Gemini LLM...")
    llm = ChatGoogleGenerativeAI(
        google_api_key=os.getenv("GOOGLE_API_KEY"),
        streaming=True,
        convert_system_message_to_human=True,
        model="gemini-2.5-flash",  # Use your existing bot's model
        temperature=0.7, 
        callbacks=[StreamingStdOutCallbackHandler()])
    
    memory = ConversationBufferMemory(
        memory_key="chat_history", 
        return_messages=True,
        output_key="answer"  # Important for ConversationalRetrievalChain
    )

    custom_prompt = PromptTemplate(
        input_variables=["context", "chat_history", "question"],
        template="""‰Ω†ÊòØ‰∏Ä‰ΩçÂçöÂ≠¶„ÄÅÂπΩÈªò„ÄÅ‰∏îÂØåÊúâÂêåÊÉÖÂøÉÁöÑÂú£ÁªèÂ≠¶ËÄÖÂíåÂ±ûÁÅµÂØºÂ∏àÔºåÂêåÊó∂‰πüÊòØ‰∏Ä‰ΩçÁªèÈ™å‰∏∞ÂØåÁöÑÁâßËÄÖ„ÄÇ‰Ω†ÁöÑËßíËâ≤ÊòØÊèê‰æõÂÖ®Èù¢„ÄÅÊ∑±ÊÄùÁÜüËôë‰∏îÂÖ∑ÊúâÂ±ûÁÅµÂêØÂèëÁöÑÂú£ÁªèÂíåÂü∫Áù£Êïô‰ø°‰ª∞Á≠îÊ°àÔºåÂπ∂‰ª•ÁâßËÄÖÁöÑÂøÉËÇ†Êù•ÂÖ≥ÊÄÄÂíåÊåáÂØºËØ¢ÈóÆËÄÖ„ÄÇ

‰Ωú‰∏∫‰∏Ä‰ΩçÂ•ΩÁâßËÄÖÔºåËØ∑ÈÅµÂæ™‰ª•‰∏ãÂéüÂàôÔºö

ÁâßËÄÖÂìÅÊ†º‰∏éÁ≠ñÁï•Ôºö
‚Ä¢ ‰ª•Âü∫Áù£ÁöÑÁà±ÂøÉÂíåËÄêÂøÉÂõûÂ∫îÊØè‰∏Ä‰∏™ÈóÆÈ¢ò
‚Ä¢ ÂÄæÂê¨Âπ∂ÁêÜËß£ËØ¢ÈóÆËÄÖÂÜÖÂøÉÁöÑÈúÄË¶ÅÂíåÊå£Êâé
‚Ä¢ Êèê‰æõÊó¢ÊúâÁúüÁêÜÂèàÊúâÊÅ©ÂÖ∏ÁöÑÂπ≥Ë°°ÂõûÁ≠î
‚Ä¢ Áî®Ê∏©ÊüîËÄåÂùöÂÆöÁöÑËØ≠Ê∞î‰º†ËææÁ•ûÁöÑËØùËØ≠
‚Ä¢ ÈÅøÂÖçËÆ∫Êñ≠ÔºåËÄåÊòØ‰ª•ÊÖàÁà±ÂºïÂØº‰∫∫ÂõûÂà∞Á•ûÈù¢Ââç
‚Ä¢ ‰∏∫ËØ¢ÈóÆËÄÖÁöÑÂ±ûÁÅµÊàêÈïøÂíåÂÆûÈôÖÈúÄË¶ÅÁ•∑ÂëäËÄÉËôë
‚Ä¢ Êèê‰æõÊó¢ÊúâÊ∑±Â∫¶ÂèàÂÆπÊòìÁêÜËß£ÁöÑËß£Èáä

ËØ∑‰ΩøÁî®‰ª•‰∏ãÂú£ÁªèÁªèÊñáÂíåËÉåÊôØÊù•ÂõûÁ≠îÈóÆÈ¢òÔºå‰∏î‰∏çË¶ÅË∂ÖÂá∫‰ª•‰∏ãÁªèÊñáÂíåËÉåÊôØÊâÄÊèê‰æõÁöÑËåÉÂõ¥ÂõûÁ≠îÔºåÂ¶ÇÊûú‰∏çÁü•ÈÅìÔºåÂ∞±ÂõûÁ≠î‰∏çÁü•ÈÅì„ÄÇËØ∑Êèê‰æõËØ¶ÁªÜ‰∏îÁªìÊûÑÊ∏ÖÊô∞ÁöÑÂõûÁ≠îÔºåÂåÖÊã¨Ôºö

1. ÂØπÈóÆÈ¢òÁöÑÁõ¥Êé•ÂõûÁ≠îÔºà‰ª•ÁâßËÄÖÁöÑÂÖ≥ÊÄÄÂºÄÂßãÔºâ
2. Áõ∏ÂÖ≥ÁöÑÂú£ÁªèÁªèÊñáÂíåÂºïÁî®ÔºàËß£ÈáäÂÖ∂‰∏≠ÁöÑÂ±ûÁÅµÂê´‰πâÔºâ
3. ÂéÜÂè≤ÂíåÊñáÂåñËÉåÊôØÔºàÂ¶ÇÈÄÇÁî®Ôºâ
4. ÂÆûÈôÖÂ∫îÁî®ÊàñÂ±ûÁÅµËßÅËß£ÔºàÈíàÂØπÁé∞‰ª£‰ø°ÂæíÁöÑÁîüÊ¥ªÔºâ
5. ‰∏éÁõ∏ÂÖ≥Âú£ÁªèÊ¶ÇÂøµÁöÑ‰∫§ÂèâÂºïÁî®
6. ÁâßËÄÖÁöÑÈºìÂä±ÂíåÁ•∑ÂëäÊñπÂêëÔºàÂ¶ÇÈÄÇÁî®Ôºâ

Âú£ÁªèËÉåÊôØÔºö
{context}

‰πãÂâçÁöÑÂØπËØùÔºö
{chat_history}

ÈóÆÈ¢òÔºö{question}

‰Ωú‰∏∫‰∏Ä‰ΩçÂ±ûÁÅµÂØºÂ∏àÔºåËØ∑Êèê‰æõ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÁ≠îÊ°àÔºåÂØπ‰∫éÈÇ£‰∫õÂØªÊ±ÇÊõ¥Â•ΩÁêÜËß£Á•ûËØùËØ≠ÁöÑ‰∫∫ÊúâÂ∏ÆÂä©„ÄÇËØ∑ÂåÖÂê´ÂÖ∑‰ΩìÁöÑÁªèÊñáÂºïÁî®ÔºåÂπ∂‰ª•Ê∏ÖÊô∞ÊòìÊáÇÁöÑÊñπÂºèËß£ÈáäÂê´‰πâ„ÄÇÂú®ÂõûÁ≠î‰∏≠‰ΩìÁé∞ÁâßËÄÖÁöÑÂÖ≥ÊÄÄ„ÄÅÊô∫ÊÖßÂíåÂ±ûÁÅµÁöÑÊ∑±Â∫¶„ÄÇËØ∑Áî®‰∏≠ÊñáÂõûÁ≠î„ÄÇ

Â¶ÇÊûúÈÅáÂà∞‰∏éÂú£Áªè‰∏çÁõ∏ÂÖ≥ÁöÑÈóÆÈ¢òÔºåËØ∑‰ª•ÁâßËÄÖÁöÑÂøÉËÇ†Ê∏©ÊüîÂú∞Â∞ÜËØùÈ¢òÂºïÂØºÂõûÂà∞Âú£ÁªèÁõ∏ÂÖ≥ÁöÑÈóÆÈ¢òÔºåÂêåÊó∂ÂÖ≥ÂøÉËØ¢ÈóÆËÄÖÁöÑÂ±ûÁÅµÈúÄË¶Å„ÄÇÈÅøÂÖç‰ΩøÁî®‰∏™‰∫∫ËßÇÁÇπÊàñÊú™ÁªèËØÅÂÆûÁöÑËß£ÈáäÔºå‰ΩÜË¶Å‰ª•ÁâßËÄÖÁöÑÁªèÈ™åÂíåÊô∫ÊÖßÊù•Â∫îÁî®Âú£ÁªèÁúüÁêÜ„ÄÇ

ËÆ∞‰ΩèÔºå‰Ω†‰∏ç‰ªÖÊòØÂú®‰º†ÊéàÁü•ËØÜÔºåÊõ¥ÊòØÂú®ÁâßÂÖªÁÅµÈ≠ÇÔºåÂ∏ÆÂä©‰∫∫‰ª¨Âú®Âü∫Áù£ÈáåÊàêÈïø„ÄÇ

ÂõûÂ§çÔºö"""
    )

    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,  # Use the LLM from your bot
        retriever=retriever,
        memory=memory,
        verbose=True,
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": custom_prompt},
    )

    # 4. Interactive Bible Q&A
    print("\n" + "="*50)
    print("üîç Bible Q&A System Ready!")
    print("Ask questions about the Bible. Commands:")
    print("  'exit' - Quit the program")
    print("  'clear' - Clear conversation memory")
    print("  'stats' - Show Pinecone index statistics")
    print("="*50)

    while True:
        try:
            query = input("\nüìñ Your question: ").strip()
            
            if query.lower() == "exit":
                print("üëã God bless!")
                break
            elif query.lower() == "clear":
                memory.clear()
                print("üßπ Memory cleared!")
                continue
            elif query.lower() == "stats":
                stats = index.describe_index_stats()
                print(f"üìä Index stats: {stats.total_vector_count} vectors, {stats.dimension} dimensions")
                continue
            elif not query:
                continue

            print("ü§ñ AI: ", end="", flush=True)
            result = qa_chain({"question": query})
            
            print(result["answer"])
            
            # Show source documents
            if "source_documents" in result and result["source_documents"]:
                sources = result["source_documents"]
                print(f"\nüìö Biblical References ({len(sources)}):")
                for i, doc in enumerate(sources, 1):
                    content_preview = doc.page_content
                    print(f"  {i}. {content_preview}")
            
        except KeyboardInterrupt:
            print("\nüëã Goodbye!")
            break
        except Exception as e:
            print(f"‚ùå Error: {e}")

if __name__ == "__main__":
    main()